{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataReader:\n",
    "    def __init__(self, path, seq_length):\n",
    "        self.fp = open(path, 'r')\n",
    "        self.data = self.fp.read()\n",
    "        #self.fp.close()\n",
    "        self.seq_length = seq_length\n",
    "        #find unique characters\n",
    "        self.data = self.data.split()\n",
    "        chars = list(set(self.data))\n",
    "        self.vocab_size = len(chars)\n",
    "        #dictionary to map characters and integers to each other\n",
    "        self.char_to_int = {ch:i for i,ch in enumerate(chars)}\n",
    "        self.int_to_char = {i:ch for i,ch in enumerate(chars)}\n",
    "        self.data_length = len(self.data)\n",
    "        self.pointer = 0\n",
    "        '''skipping something will look later'''\n",
    "    def close(self):\n",
    "        self.fp.close()\n",
    "    \n",
    "    def next_batch(self):\n",
    "        input_start = self.pointer\n",
    "        input_end = self.pointer + self.seq_length\n",
    "        inputs = [self.char_to_int[ch] for ch in self.data[input_start:input_end]]\n",
    "        #for each input word, the target is the next word\n",
    "        targets = [self.char_to_int[ch] for ch in self.data[input_start+1:input_end+1]]\n",
    "        self.pointer += self.seq_length\n",
    "        if self.pointer + self.seq_length + 1 >= self.data_length:\n",
    "            self.pointer = 0\n",
    "        return inputs, targets\n",
    "    \n",
    "    def just_started(self):\n",
    "        return self.pointer == 0\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*   Initialization of weights have an impact on the training of the model, so we have to choose initialization according to our activation function. In our case we will be using tanh as activation function, so **Xavier initialization** is the best choice for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN:\n",
    "    def __init__(self, hidden_size, vocab_size, seq_length, learning_rate):\n",
    "        #hyper parameters\n",
    "        self.hidden_size = hidden_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.seq_length = seq_length\n",
    "        self.learning_rate = learning_rate\n",
    "        #model parameters are initialized using Xavier initialization\n",
    "        self.U = np.random.uniform(-np.sqrt(1./vocab_size), np.sqrt(1./vocab_size), (hidden_size, vocab_size))\n",
    "        self.V = np.random.uniform(-np.sqrt(1./hidden_size), np.sqrt(1./hidden_size), (vocab_size, hidden_size))\n",
    "        self.W = np.random.uniform(-np.sqrt(1./hidden_size), np.sqrt(1./hidden_size), (hidden_size, hidden_size))\n",
    "        self.b = np.zeros((hidden_size, 1)) #bias for hidden layer\n",
    "        self.c = np.zeros((vocab_size, 1)) #bias for output layer\n",
    "\n",
    "        #memory variables for adagrad\n",
    "        self.mU = np.zeros_like(self.U)\n",
    "        self.mW = np.zeros_like(self.W)\n",
    "        self.mV = np.zeros_like(self.V)\n",
    "        self.mb = np.zeros_like(self.b)\n",
    "        self.mc = np.zeros_like(self.c)\n",
    "\n",
    "    #define the softmax function\n",
    "    def softmax(self, x):\n",
    "        e_x = np.exp(x - np.max(x))\n",
    "        return e_x / e_x.sum(axis=0)\n",
    "    \n",
    "    #defining the froward pass\n",
    "    def forward(self, inputs, hprev):\n",
    "        xs ,hs, os, ycap = {}, {}, {}, {}\n",
    "        hs[-1] = np.copy(hprev)\n",
    "        for t in range(len(inputs)):\n",
    "            xs[t] = np.zeros((self.vocab_size, 1))\n",
    "            xs[t][inputs[t]] = 1 #one hot encoding\n",
    "            hs[t] = np.tanh(np.dot(self.U, xs[t]) + np.dot(self.W, hs[t-1]) + self.b)\n",
    "            os[t] = np.dot(self.V, hs[t]) + self.c\n",
    "            ycap[t] = self.softmax(os[t])\n",
    "        return xs, hs, ycap\n",
    "    \n",
    "    #defining the backward pass\n",
    "    def backward(self, xs, hs, ps, targets):\n",
    "        #initialize the gradients\n",
    "        dU, dW, dV = np.zeros_like(self.U), np.zeros_like(self.W), np.zeros_like(self.V)\n",
    "        db, dc = np.zeros_like(self.b), np.zeros_like(self.c)\n",
    "        dhnext = np.zeros_like(hs[0])\n",
    "        #backpropagate through time in reverse\n",
    "        for t in reversed(range(self.seq_length)):\n",
    "            dy = np.copy(ps[t])\n",
    "            #gradient through the softmax layer\n",
    "            dy[targets[t]] -= 1\n",
    "            #calculating the gradients wrt V and c\n",
    "            dV += np.dot(dy, hs[t].T)\n",
    "            dc += dy\n",
    "            #backpropagate the gradient to the hidden layer\n",
    "            #dh includes gradient form two sides, from the output layer and from the next time step\n",
    "            dh = np.dot(self.V.T, dy) + dhnext\n",
    "            #backpropagate through the tanh\n",
    "            dhraw = (1 - hs[t] * hs[t]) * dh\n",
    "            db += dhraw\n",
    "            #calculate the gradients wrt U and W\n",
    "            dU += np.dot(dhraw, xs[t].T)\n",
    "            dW += np.dot(dhraw, hs[t-1].T)\n",
    "            #calculate the gradient wrt the next hidden state\n",
    "            dhnext = np.dot(self.W.T, dhraw)\n",
    "        #clip the gradients to mitigate the exploding gradients problem\n",
    "        for dparam in [dU, dW, dV, db, dc]:\n",
    "            np.clip(dparam, -5, 5, out=dparam)\n",
    "        return dU, dW, dV, db, dc\n",
    "    \n",
    "    #define the cross entropy loss for one predicted sequence\n",
    "    def loss(self, ps, targets):\n",
    "        '''here we are using the negative log likelihood loss(just another name for cross entropy loss)'''\n",
    "        return sum(-np.log(ps[t][targets[t], 0]) for t in range(self.seq_length))\n",
    "    \n",
    "    #define the update function\n",
    "    def update(self, dU, dW, dV, db, dc):\n",
    "        #updating parameters using adagrad\n",
    "        for param, dparam, mem in zip([self.U, self.W, self.V, self.b, self.c], [dU, dW, dV, db, dc], [self.mU, self.mW, self.mV, self.mb, self.mc]):\n",
    "            mem += dparam * dparam\n",
    "            param += -self.learning_rate * dparam / np.sqrt(mem + 1e-8)\n",
    "\n",
    "    def train(self, data_reader):\n",
    "        iter_num = 1\n",
    "        threshold = 0.01\n",
    "        training_loss = []\n",
    "        '''fundamentals of smooth loss are not clear yet'''\n",
    "        #smooth_loss = -np.log(1.0/data_reader.vocab_size)*self.seq_length\n",
    "        for i in range(10000):\n",
    "            plt.clf()\n",
    "            if data_reader.just_started():\n",
    "                hprev = np.zeros((self.hidden_size, 1))\n",
    "            #creating the input and target sequences of length seq_length for training\n",
    "            inputs, targets = data_reader.next_batch()\n",
    "            #forward pass\n",
    "            xs, hs, ps = self.forward(inputs, hprev)\n",
    "            #backward pass\n",
    "            dU, dW, dV, db, dc = self.backward(xs, hs, ps, targets)\n",
    "            #calculate the loss\n",
    "            loss = self.loss(ps, targets)\n",
    "            training_loss.append(loss)\n",
    "            #update the model parameters\n",
    "            self.update(dU, dW, dV, db, dc)\n",
    "            #update the hidden state\n",
    "            hprev = hs[self.seq_length-1]\n",
    "            plt.plot(training_loss, label=\"Train Loss\")\n",
    "            plt.xlabel(\"Iterations\")\n",
    "            plt.ylabel(\"Loss\")\n",
    "            plt.title(\"Training Loss Curve\")\n",
    "            plt.legend()\n",
    "            display.display(plt.gcf())\n",
    "            display.clear_output(wait=True)\n",
    "            time.sleep(0.01)          \n",
    "            print('iter %d, loss: %f' % (iter_num, loss))\n",
    "            time.sleep(0.001)\n",
    "            iter_num += 1\n",
    "    #define function for predicting the next sequence\n",
    "    def predict(self, data_reader, start, n):\n",
    "        start = start.split()\n",
    "        #initialize the input vector\n",
    "        x = np.zeros((self.vocab_size, 1))\n",
    "        chars = [ch for ch in start]\n",
    "        ixes = []\n",
    "        for i in range(len(chars)):\n",
    "            ix = data_reader.char_to_int[chars[i]]\n",
    "            x[ix] = 1\n",
    "            ixes.append(ix)\n",
    "        #initialize the hidden state\n",
    "        h = np.zeros((self.hidden_size, 1))\n",
    "        #predict the next n characters\n",
    "        for t in range(n):\n",
    "            h= np.tanh(np.dot(self.U, x) + np.dot(self.W, h) + self.b)\n",
    "            o = np.dot(self.V, h) + self.c\n",
    "            p = self.softmax(o)\n",
    "            #predict the index of the next character\n",
    "            ix = np.random.choice(range(self.vocab_size), p=p.ravel())\n",
    "            #update the input vector\n",
    "            x = np.zeros((self.vocab_size, 1))\n",
    "            x[ix] = 1\n",
    "            ixes.append(ix)\n",
    "        #generate the text with spaces between words\n",
    "        txt = ' '.join(data_reader.int_to_char[ix] for ix in ixes)\n",
    "        return txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = DataReader('RNN_input.txt', 15)\n",
    "print(data.vocab_size)\n",
    "#print(data.data[0])\n",
    "#print(data.char_to_int)\n",
    "#print(data.int_to_char)\n",
    "rnn = RNN(hidden_size=100, vocab_size=data.vocab_size,seq_length=15,learning_rate=0.1)\n",
    "rnn.train(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn.predict(data, 'rainy', 15)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
